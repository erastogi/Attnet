{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sys import stdout\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import progressbar\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=stdout)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('/mnt/W/Users/Alt/Documents/CMU/11777/data/MSCOCO/annotations/captions_val2014.json',\n",
    "                  typ='series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [entry['caption'].rstrip() for entry in df['annotations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_captions = [word_tokenize(caption) for caption in captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(word for caption in tokenized_captions for word in caption)\n",
    "encoder = {word: code for code, word in enumerate(sorted(vocabulary))}\n",
    "X = [np.array([encoder[word] for word in caption]) for caption in tokenized_captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(vocabulary)\n",
    "Ik = np.eye(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired from https://chsasank.github.io/spoken-language-understanding.html and https://stackoverflow.com/questions/39142665/keras-lstm-language-model-using-embeddings\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=k,\n",
    "                    output_dim=50))\n",
    "model.add(SimpleRNN(units=50,\n",
    "                    return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(units=k, activation='softmax')))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-09 18:00:06,349 | INFO : Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |#######################| Elapsed Time: 0:00:07 Time: 0:00:07\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "X_subset = X[:100]\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    logger.info('Training epoch %d', i)\n",
    "    bar = progressbar.ProgressBar()\n",
    "    for caption_repr in bar(X_subset):\n",
    "        if len(caption_repr) > 1:\n",
    "            input_ = caption_repr[:-1][np.newaxis, :]\n",
    "            label = Ik[caption_repr[1:]][np.newaxis, :]\n",
    "            model.train_on_batch(input_,\n",
    "                                 label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
